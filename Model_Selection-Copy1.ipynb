{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cba607f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import itertools as it\n",
    "from dataclasses import dataclass\n",
    "from functools import wraps\n",
    "import numpy as np\n",
    "import pandas as pd          \n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri, numpy2ri\n",
    "import os\n",
    "from rpy2.robjects.vectors import FloatVector\n",
    "from lassonet import LassoNetRegressor, LassoNetClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from numpy import log as ln\n",
    "import itertools as it\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r'Environment variable \".*\" redefined by R and overriding existing variable\\.',\n",
    "    category=UserWarning,\n",
    "    module=r\"rpy2\\.rinterface.*\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7217ac7d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Combined converter (no global activate/deactivate)\n",
    "_CONV = ro.default_converter + pandas2ri.converter + numpy2ri.converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31a51349",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <span>ListVector with 2 elements.</span>\n",
       "        <table>\n",
       "        <tbody>\n",
       "        \n",
       "          <tr>\n",
       "            <th>\n",
       "            value\n",
       "            </th>\n",
       "            <td>\n",
       "            <rpy2.rinterface.SexpClosure object at 0x7f22c11bf310> [3]\n",
       "            </td>\n",
       "          </tr>\n",
       "        \n",
       "          <tr>\n",
       "            <th>\n",
       "            visible\n",
       "            </th>\n",
       "            <td>\n",
       "            <rpy2.rinterface.BoolSexpVector object at 0x7f22c11bfe50> [10]\n",
       "            </td>\n",
       "          </tr>\n",
       "        \n",
       "        </tbody>\n",
       "        </table>\n",
       "        "
      ],
      "text/plain": [
       "<rpy2.robjects.vectors.ListVector object at 0x7f22c11bcd10> [19]\n",
       "R classes: ('list',)\n",
       "[SexpClosure, BoolSexpVector]\n",
       "  value: <class 'rpy2.rinterface.SexpClosure'>\n",
       "  <rpy2.rinterface.SexpClosure object at 0x7f22c11be190> [3]\n",
       "  visible: <class 'rpy2.rinterface.BoolSexpVector'>\n",
       "  <rpy2.rinterface.BoolSexpVector object at 0x7f22c11bf310> [10]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source Init.R that holds the R functions\n",
    "script = Path(\"Init.R\").resolve()\n",
    "ro.r['source'](script.as_posix())\n",
    "\n",
    "# in .py Datei\n",
    "# script = Path(__file__).with_name(\"Init.R\").resolve()\n",
    "# ro.r['source'](file=script.as_posix(), chdir=True, encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c1d9276",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# R-Funktionen in Python-Handles überführen\n",
    "stepwise_plain_r   = ro.globalenv[\"stepwise_plain\"]\n",
    "stepwise_reduced_r = ro.globalenv[\"stepwise_reduced\"]\n",
    "stepwise_ff_r      = ro.globalenv[\"stepwise_ff\"]\n",
    "L0opt_CD_r         = ro.globalenv[\"L0opt_CD\"]\n",
    "L0opt_CDPSI_r      = ro.globalenv[\"L0opt_CDPSI\"]\n",
    "Select_GSDAR_r     = ro.globalenv[\"Select_GSDAR\"]\n",
    "mbic_r  = ro.globalenv[\"mbic_py\"]\n",
    "mbic2_r = ro.globalenv[\"mbic2_py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bf83635",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Structure of the results \n",
    "@dataclass\n",
    "class ModelSelResult:\n",
    "    mBIC:   float\n",
    "    mBIC2:  float\n",
    "    model1: np.ndarray   # Indizes (1-basiert, wie in R) des mBIC-Optimums\n",
    "    model2: np.ndarray   # Indizes des mBIC2-Optimums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10e0e6e5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def _to_r_numeric_vector(arr):\n",
    "    \"\"\"Return an R 'numeric' vector without dimensions.\"\"\"\n",
    "    return FloatVector(np.asarray(arr, dtype=float).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bb7400f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Dekorator, der den lokalen Converter ein- und ausschaltet\n",
    "def _with_conversion(func):\n",
    "    @wraps(func)\n",
    "    def _wrapper(*args, **kwargs):\n",
    "        with _CONV.context():\n",
    "            return func(*args, **kwargs)\n",
    "    return _wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2b10f83-8b85-4fdc-98c7-acdd1a361adb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "@_with_conversion\n",
    "def _mbic_bigstep(loglik: float, n: int, k: int, p: int, const: float = 4.0) -> float:\n",
    "    return float(np.asarray(mbic_r(loglik, n, k, p, const=const))[0])\n",
    "\n",
    "@_with_conversion\n",
    "def _mbic2_bigstep(loglik: float, n: int, k: int, p: int, const: float = 4.0) -> float:\n",
    "    return float(np.asarray(mbic2_r(loglik, n, k, p, const=const))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e29bfefb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Python-freundliche Wrapper\n",
    "@_with_conversion\n",
    "def stepwise_plain(y: np.ndarray,\n",
    "                   X: np.ndarray,\n",
    "                   model: str = \"linear\") -> ModelSelResult:\n",
    "    y_r = _to_r_numeric_vector(y)\n",
    "    res = stepwise_plain_r(y_r, X, model)\n",
    "    return ModelSelResult(\n",
    "        mBIC=float(res[0]),\n",
    "        mBIC2=float(res[1]),\n",
    "        model1=np.asarray(res[2], dtype=int),\n",
    "        model2=np.asarray(res[3], dtype=int),\n",
    "    )\n",
    "\n",
    "@_with_conversion\n",
    "def stepwise_reduced(y: np.ndarray,\n",
    "                     X: np.ndarray,\n",
    "                     model: str = \"linear\") -> ModelSelResult:\n",
    "    y_r = _to_r_numeric_vector(y)\n",
    "    res = stepwise_reduced_r(y_r, X, model)\n",
    "    return ModelSelResult(\n",
    "        mBIC=float(res[0]),\n",
    "        mBIC2=float(res[1]),\n",
    "        model1=np.asarray(res[2], dtype=int),\n",
    "        model2=np.asarray(res[3], dtype=int),\n",
    "    )\n",
    "\n",
    "@_with_conversion\n",
    "def stepwise_ff(y: np.ndarray,\n",
    "                X: np.ndarray,\n",
    "                model: str = \"linear\") -> ModelSelResult:\n",
    "    y_r = _to_r_numeric_vector(y)\n",
    "    res = stepwise_ff_r(y_r, X, model)\n",
    "    return ModelSelResult(\n",
    "        mBIC=float(res[0]),\n",
    "        mBIC2=float(res[1]),\n",
    "        model1=np.asarray(res[2], dtype=int),\n",
    "        model2=np.asarray(res[3], dtype=int),\n",
    "    )\n",
    "\n",
    "@_with_conversion\n",
    "def L0opt_CD(\n",
    "        y: np.ndarray,\n",
    "        X: np.ndarray,\n",
    "        model: str = \"SquaredError\",\n",
    "        maxSuppSize: int | None = None,\n",
    ") -> ModelSelResult:\n",
    "    y_r = _to_r_numeric_vector(y)\n",
    "\n",
    "    if maxSuppSize is None:\n",
    "        # Ohne Argument aufrufen – R nimmt dann maxSuppSize = NA\n",
    "        res = L0opt_CD_r(y_r, X, model)\n",
    "    else:\n",
    "        # Wert explizit übergeben\n",
    "        res = L0opt_CD_r(y_r, X, model, maxSuppSize=maxSuppSize)\n",
    "\n",
    "    return ModelSelResult(\n",
    "        mBIC=float(res[0]),\n",
    "        mBIC2=float(res[1]),\n",
    "        model1=np.asarray(res[2], dtype=int),\n",
    "        model2=np.asarray(res[3], dtype=int),\n",
    "    )\n",
    "\n",
    "@_with_conversion\n",
    "def L0opt_CDPSI(\n",
    "        y: np.ndarray,\n",
    "        X: np.ndarray,\n",
    "        model: str = \"SquaredError\",\n",
    "        maxSuppSize: int | None = None,\n",
    ") -> ModelSelResult:\n",
    "    y_r = _to_r_numeric_vector(y)\n",
    "\n",
    "    if maxSuppSize is None:\n",
    "        res = L0opt_CDPSI_r(y_r, X, model)\n",
    "    else:\n",
    "        res = L0opt_CDPSI_r(y_r, X, model, maxSuppSize=maxSuppSize)\n",
    "\n",
    "    return ModelSelResult(\n",
    "        mBIC=float(res[0]),\n",
    "        mBIC2=float(res[1]),\n",
    "        model1=np.asarray(res[2], dtype=int),\n",
    "        model2=np.asarray(res[3], dtype=int),\n",
    "    )\n",
    "\n",
    "@_with_conversion\n",
    "def Select_GSDAR(y: np.ndarray,\n",
    "                 X: np.ndarray) -> ModelSelResult:\n",
    "    y_r = _to_r_numeric_vector(y)\n",
    "    res = Select_GSDAR_r(y_r, X)\n",
    "    return ModelSelResult(\n",
    "        mBIC=float(res[0]),\n",
    "        mBIC2=float(res[1]),\n",
    "        model1=np.asarray(res[2], dtype=int),\n",
    "        model2=np.asarray(res[3], dtype=int),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49e4b637-4c2e-48e1-b071-505a8de9587c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def lassonet(\n",
    "    y: np.ndarray,\n",
    "    X: np.ndarray,\n",
    "    model: str = \"linear\",\n",
    "    *,\n",
    "    M: int | None = None,\n",
    "    path_multiplier: float | None = None,\n",
    "    hidden_dims: tuple[int, ...] | None = None,\n",
    "    n_iters: int | None = None,\n",
    "    early_patience: int | None = None,\n",
    "    no_improve_patience: int | None = None,\n",
    "    random_state: int | None = None,\n",
    "    device: str | None = None,\n",
    ") -> ModelSelResult:\n",
    "    \"\"\"\n",
    "    LassoNet-Pfad auswerten und bestes Modell (mBIC/mBIC2) wählen.\n",
    "    Verbesserungssignal = (mBIC besser) ODER (k = |Support| sinkt).\n",
    "    Abbruch nach 'no_improve_patience' aufeinanderfolgenden Supports ohne Verbesserungssignal.\n",
    "    \"\"\"\n",
    "    # ---- Defaults (wie bisher) ----\n",
    "    NO_IMPROVE_PATIENCE = 4 if no_improve_patience is None else no_improve_patience\n",
    "    TOL = 1e-9\n",
    "    SAT_DELTA = 5\n",
    "    HIDDEN_DIMS = (4,) if hidden_dims is None else hidden_dims\n",
    "    PATH_MULTIPLIER = 1.08 if path_multiplier is None else path_multiplier\n",
    "    N_ITERS = 100 if n_iters is None else n_iters\n",
    "    EARLY_PATIENCE = 2 if early_patience is None else early_patience\n",
    "    RANDOM_STATE = 42 if random_state is None else random_state\n",
    "    M = 30 if M is None else M\n",
    "    DEVICE = \"cpu\" if device is None else device\n",
    "    BATCH_SIZE = min(X.shape[0], 512)\n",
    "\n",
    "    # ---- Vorbereitungen ----\n",
    "    model = model.lower().strip()\n",
    "    if model not in (\"linear\", \"logistic\"):\n",
    "        raise ValueError(\"lassonet: model must be 'linear' or 'logistic'.\")\n",
    "    is_logistic = (model == \"logistic\")\n",
    "\n",
    "    n, p = X.shape\n",
    "    SAT_CUTOFF = min(int(p/2),int(n/2),150)\n",
    "\n",
    "    def _ll_full(supp: np.ndarray) -> tuple[float, int]:\n",
    "        \"\"\"Log-Likelihood für gegebenen Support.\"\"\"\n",
    "        k = supp.size\n",
    "        if k == 0:\n",
    "            if is_logistic:\n",
    "                p_hat = float(y.mean())\n",
    "                p_hat = min(max(p_hat, 1e-12), 1 - 1e-12)\n",
    "                ll = float((y * np.log(p_hat) + (1 - y) * np.log(1 - p_hat)).sum())\n",
    "            else:\n",
    "                sigma2_0 = max(float(np.var(y, ddof=0)), 1e-12)\n",
    "                ll = -0.5 * n * np.log(sigma2_0)\n",
    "            return ll, 0\n",
    "\n",
    "        Xk = X[:, supp]\n",
    "        if is_logistic:\n",
    "            mdl = LogisticRegression(penalty=None, solver=\"lbfgs\", max_iter=2000).fit(Xk, y.astype(int))\n",
    "            proba = np.clip(mdl.predict_proba(Xk)[:, 1], 1e-12, 1 - 1e-12)\n",
    "            ll = float((y * np.log(proba) + (1 - y) * np.log(1 - proba)).sum())\n",
    "        else:\n",
    "            mdl = LinearRegression().fit(Xk, y)\n",
    "            rss = float(((mdl.predict(Xk) - y) ** 2).sum())\n",
    "            sigma2 = max(rss / n, 1e-12)\n",
    "            ll = -0.5 * n * np.log(sigma2)\n",
    "        return ll, k\n",
    "\n",
    "    net_cls = LassoNetClassifier if is_logistic else LassoNetRegressor\n",
    "    net = net_cls(\n",
    "        hidden_dims=HIDDEN_DIMS,\n",
    "        path_multiplier=PATH_MULTIPLIER,\n",
    "        lambda_start=\"auto\",\n",
    "        n_iters=N_ITERS,\n",
    "        patience=EARLY_PATIENCE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        val_size=0.0,\n",
    "        random_state=RANDOM_STATE,\n",
    "        M=M,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "\n",
    "    X_path = X * float(np.sqrt(n))\n",
    "    y_fit = y.astype(int) if is_logistic else y\n",
    "\n",
    "    best_mbic = float(\"inf\");   best_sup_mbic = np.array([], dtype=int);   patience_mbic = 0\n",
    "    best_mbic2 = float(\"inf\");  best_sup_mbic2 = np.array([], dtype=int);  patience_mbic2 = 0\n",
    "    best_k_for_mbic  = np.inf\n",
    "    best_k_for_mbic2 = np.inf\n",
    "\n",
    "    seen_supports: set[tuple[int, ...]] = set()\n",
    "\n",
    "    for _, ckpt in enumerate(net.path(X_path, y_fit)):\n",
    "        selected = getattr(ckpt, \"selected\", getattr(ckpt, \"selected_features_\", None))\n",
    "        if selected is None:\n",
    "            continue\n",
    "\n",
    "        mask = np.asarray(selected, dtype=bool)\n",
    "        df = int(mask.sum())\n",
    "        if df >= SAT_CUTOFF:\n",
    "            continue\n",
    "\n",
    "        supp = np.flatnonzero(mask)\n",
    "        key = tuple(supp.tolist())\n",
    "        if key in seen_supports:\n",
    "            continue\n",
    "        seen_supports.add(key)\n",
    "\n",
    "        ll, k = _ll_full(supp)\n",
    "        mbic  = _mbic_bigstep(ll, n, k, p)\n",
    "        mbic2 = _mbic2_bigstep(ll, n, k, p)\n",
    "\n",
    "        # mBIC\n",
    "        improved_val = (mbic < best_mbic - TOL)\n",
    "        improved_k   = (k < best_k_for_mbic)\n",
    "        if improved_val or improved_k:\n",
    "            if improved_val:\n",
    "                best_mbic, best_sup_mbic = mbic, supp.copy()\n",
    "            best_k_for_mbic = min(best_k_for_mbic, k)\n",
    "            patience_mbic = 0\n",
    "        else:\n",
    "            patience_mbic += 1\n",
    "\n",
    "        # mBIC2\n",
    "        improved_val2 = (mbic2 < best_mbic2 - TOL)\n",
    "        improved_k2   = (k < best_k_for_mbic2)\n",
    "        if improved_val2 or improved_k2:\n",
    "            if improved_val2:\n",
    "                best_mbic2, best_sup_mbic2 = mbic2, supp.copy()\n",
    "            best_k_for_mbic2 = min(best_k_for_mbic2, k)\n",
    "            patience_mbic2 = 0\n",
    "        else:\n",
    "            patience_mbic2 += 1\n",
    "\n",
    "        if patience_mbic >= NO_IMPROVE_PATIENCE and patience_mbic2 >= NO_IMPROVE_PATIENCE:\n",
    "            break\n",
    "\n",
    "    mBIC_fin  = float(best_mbic)  if np.isfinite(best_mbic)  else np.nan\n",
    "    mBIC2_fin = float(best_mbic2) if np.isfinite(best_mbic2) else np.nan\n",
    "\n",
    "    return ModelSelResult(\n",
    "        mBIC=mBIC_fin,\n",
    "        mBIC2=mBIC2_fin,\n",
    "        model1=best_sup_mbic + 1,    # 1-basiert\n",
    "        model2=best_sup_mbic2 + 1,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa269761-ab2d-4aeb-890a-fe8879804853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lassonet_plus(\n",
    "    y: np.ndarray,\n",
    "    X: np.ndarray,\n",
    "    model: str = \"linear\",\n",
    ") -> ModelSelResult:\n",
    "    \"\"\"\n",
    "    LassoNet mit identischen Hyperparametern wie `lassonet`, aber erweiterter\n",
    "    mBIC-/mBIC2-gesteuerter Nachbearbeitung:\n",
    "      - Top-K auf Basis feature_importances_ + Feinsuche\n",
    "      - Großer Korrelations-Screen\n",
    "      - Residual-Refinement (Add, Backward-Prune, Swap) für linear & logistic\n",
    "    Gibt wie gehabt mBIC/mBIC2 und die beiden Modelle (1-basiert) zurück.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- Parameter (IDENTISCH zu `lassonet`) ----\n",
    "    NO_IMPROVE_PATIENCE = 4\n",
    "    TOL = 1e-9\n",
    "    SAT_DELTA = 5\n",
    "    HIDDEN_DIMS = (4,)\n",
    "    PATH_MULTIPLIER = 1.08\n",
    "    N_ITERS = 100\n",
    "    EARLY_PATIENCE = 2\n",
    "    BATCH_SIZE = min(X.shape[0], 512)\n",
    "    RANDOM_STATE = 42\n",
    "    M = 30\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "    # ---- Vorbereitungen ----\n",
    "    model = model.lower().strip()\n",
    "    if model not in (\"linear\", \"logistic\"):\n",
    "        raise ValueError(\"lassonet_plus: model must be 'linear' or 'logistic'.\")\n",
    "    is_logistic = (model == \"logistic\")\n",
    "\n",
    "    n, p = X.shape\n",
    "    SAT_CUTOFF = min(int(p/2), int(n/2), 150)\n",
    "    K_MAX = SAT_CUTOFF\n",
    "\n",
    "    def _ll_full(supp: np.ndarray) -> tuple[float, int]:\n",
    "        \"\"\"Log-Likelihood für gegebenen Support (identisch zur Logik in `lassonet`).\"\"\"\n",
    "        k = supp.size\n",
    "        if k == 0:\n",
    "            if is_logistic:\n",
    "                p_hat = float(y.mean())\n",
    "                p_hat = min(max(p_hat, 1e-12), 1 - 1e-12)\n",
    "                ll = float((y * np.log(p_hat) + (1 - y) * np.log(1 - p_hat)).sum())\n",
    "            else:\n",
    "                sigma2_0 = max(float(np.var(y, ddof=0)), 1e-12)\n",
    "                ll = -0.5 * n * np.log(sigma2_0)\n",
    "            return ll, 0\n",
    "\n",
    "        Xk = X[:, supp]\n",
    "        if is_logistic:\n",
    "            mdl = LogisticRegression(penalty=None, solver=\"lbfgs\", max_iter=2000).fit(Xk, y.astype(int))\n",
    "            proba = np.clip(mdl.predict_proba(Xk)[:, 1], 1e-12, 1 - 1e-12)\n",
    "            ll = float((y * np.log(proba) + (1 - y) * np.log(1 - proba)).sum())\n",
    "        else:\n",
    "            mdl = LinearRegression().fit(Xk, y)\n",
    "            rss = float(((mdl.predict(Xk) - y) ** 2).sum())\n",
    "            sigma2 = max(rss / n, 1e-12)\n",
    "            ll = -0.5 * n * np.log(sigma2)\n",
    "        return ll, k\n",
    "\n",
    "    # ---- LassoNet Setup (identisch) ----\n",
    "    net_cls = LassoNetClassifier if is_logistic else LassoNetRegressor\n",
    "    net = net_cls(\n",
    "        hidden_dims=HIDDEN_DIMS,\n",
    "        path_multiplier=PATH_MULTIPLIER,\n",
    "        lambda_start=\"auto\",\n",
    "        n_iters=N_ITERS,\n",
    "        patience=EARLY_PATIENCE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        val_size=0.0,\n",
    "        random_state=RANDOM_STATE,\n",
    "        M=M,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "\n",
    "    X_path = X * float(np.sqrt(n))\n",
    "    y_fit = y.astype(int) if is_logistic else y\n",
    "\n",
    "    # ---- Pfad-Scan (identisch) ----\n",
    "    best_mbic = float(\"inf\");   best_sup_mbic = np.array([], dtype=int);   patience_mbic = 0\n",
    "    best_mbic2 = float(\"inf\");  best_sup_mbic2 = np.array([], dtype=int);  patience_mbic2 = 0\n",
    "    best_k_for_mbic  = np.inf\n",
    "    best_k_for_mbic2 = np.inf\n",
    "\n",
    "    seen_supports: set[tuple[int, ...]] = set()\n",
    "    last_state_dict = None\n",
    "\n",
    "    for _, ckpt in enumerate(net.path(X_path, y_fit)):\n",
    "        selected = getattr(ckpt, \"selected\", getattr(ckpt, \"selected_features_\", None))\n",
    "        sd = getattr(ckpt, \"state_dict\", None)\n",
    "        if sd is not None:\n",
    "            last_state_dict = sd\n",
    "        if selected is None:\n",
    "            continue\n",
    "\n",
    "        mask = np.asarray(selected, dtype=bool)\n",
    "        df = int(mask.sum())\n",
    "        if df >= SAT_CUTOFF:\n",
    "            continue\n",
    "\n",
    "        supp = np.flatnonzero(mask)\n",
    "        key = tuple(supp.tolist())\n",
    "        if key in seen_supports:\n",
    "            continue\n",
    "        seen_supports.add(key)\n",
    "\n",
    "        ll, k = _ll_full(supp)\n",
    "        mbic  = _mbic_bigstep(ll, n, k, p)\n",
    "        mbic2 = _mbic2_bigstep(ll, n, k, p)\n",
    "\n",
    "        improved_val = (mbic < best_mbic - TOL)\n",
    "        improved_k   = (k < best_k_for_mbic)\n",
    "        if improved_val or improved_k:\n",
    "            if improved_val:\n",
    "                best_mbic, best_sup_mbic = mbic, supp.copy()\n",
    "            best_k_for_mbic = min(best_k_for_mbic, k)\n",
    "            patience_mbic = 0\n",
    "        else:\n",
    "            patience_mbic += 1\n",
    "\n",
    "        improved_val2 = (mbic2 < best_mbic2 - TOL)\n",
    "        improved_k2   = (k < best_k_for_mbic2)\n",
    "        if improved_val2 or improved_k2:\n",
    "            if improved_val2:\n",
    "                best_mbic2, best_sup_mbic2 = mbic2, supp.copy()\n",
    "            best_k_for_mbic2 = min(best_k_for_mbic2, k)\n",
    "            patience_mbic2 = 0\n",
    "        else:\n",
    "            patience_mbic2 += 1\n",
    "\n",
    "        if patience_mbic >= NO_IMPROVE_PATIENCE and patience_mbic2 >= NO_IMPROVE_PATIENCE:\n",
    "            break\n",
    "\n",
    "    # ------------------------------\n",
    "    # Top-K auf Basis feature_importances_ + Feinsuche\n",
    "    # ------------------------------\n",
    "    if last_state_dict is not None:\n",
    "        try:\n",
    "            net.load_state_dict(last_state_dict)\n",
    "        except Exception:\n",
    "            try:\n",
    "                net.load(last_state_dict)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # robustes Ranking: fallback auf |X^Ty|\n",
    "    try:\n",
    "        imp_final  = np.asarray(net.feature_importances_)\n",
    "        rank_theta = np.argsort(np.abs(imp_final))[::-1]\n",
    "    except Exception:\n",
    "        rank_theta = np.argsort(np.abs(X.T @ y))[::-1]\n",
    "\n",
    "    K_hi = min(K_MAX, p)\n",
    "\n",
    "    def _score_S(S: np.ndarray):\n",
    "        ll, kk = _ll_full(S)\n",
    "        return _mbic_bigstep(ll, n, kk, p), _mbic2_bigstep(ll, n, kk, p)\n",
    "\n",
    "    def _score_K(K: int):\n",
    "        S = rank_theta[:K]\n",
    "        v1, v2 = _score_S(S)\n",
    "        return v1, v2, S\n",
    "\n",
    "    K_seed = int(best_sup_mbic.size) if best_sup_mbic.size > 0 else min(8, K_hi)\n",
    "    WINDOW = 10\n",
    "    STEP   = 2\n",
    "    K_low  = max(1, K_seed - WINDOW)\n",
    "    K_up   = min(K_seed + WINDOW, K_hi)\n",
    "\n",
    "    best_theta_mBIC,  best_theta_sup,  K_best  = best_mbic,  best_sup_mbic.copy(),  K_seed\n",
    "    best_theta_mBIC2, best_theta_sup2, K_best2 = best_mbic2, best_sup_mbic2.copy(), K_seed\n",
    "\n",
    "    for K in range(K_low, K_up + 1, STEP):\n",
    "        v1, v2, S = _score_K(K)\n",
    "        if v1 < best_theta_mBIC - TOL:\n",
    "            best_theta_mBIC, best_theta_sup,  K_best  = v1, S.copy(), K\n",
    "        if v2 < best_theta_mBIC2 - TOL:\n",
    "            best_theta_mBIC2, best_theta_sup2, K_best2 = v2, S.copy(), K\n",
    "\n",
    "    for K in range(max(1, K_best  - 3), min(K_hi, K_best  + 3) + 1):\n",
    "        v1, v2, S = _score_K(K)\n",
    "        if v1 < best_theta_mBIC - TOL:\n",
    "            best_theta_mBIC, best_theta_sup = v1, S.copy()\n",
    "\n",
    "    for K in range(max(1, K_best2 - 3), min(K_hi, K_best2 + 3) + 1):\n",
    "        v1, v2, S = _score_K(K)\n",
    "        if v2 < best_theta_mBIC2 - TOL:\n",
    "            best_theta_mBIC2, best_theta_sup2 = v2, S.copy()\n",
    "\n",
    "    if best_theta_mBIC  < best_mbic  - TOL: best_mbic,  best_sup_mbic  = best_theta_mBIC,  best_theta_sup\n",
    "    if best_theta_mBIC2 < best_mbic2 - TOL: best_mbic2, best_sup_mbic2 = best_theta_mBIC2, best_theta_sup2\n",
    "\n",
    "\n",
    "    # ------------------------------\n",
    "    # Großer Korrelations-Screen \n",
    "    # ------------------------------\n",
    "    K_screen = min(K_MAX, 70)\n",
    "    scores_glob = np.abs(X.T @ y)\n",
    "    ranked = np.argsort(scores_glob)[::-1][:K_screen]\n",
    "\n",
    "    Ks_scr = [1]\n",
    "    while Ks_scr[-1] < K_screen:\n",
    "        Ks_scr.append(min(K_screen, Ks_scr[-1] * 2))\n",
    "    if best_sup_mbic.size > 0:\n",
    "        Ks_scr.append(min(K_screen, best_sup_mbic.size))\n",
    "    Ks_scr = sorted(set(Ks_scr))\n",
    "\n",
    "    best_scr_mBIC,  best_scr_sup  = best_mbic,  best_sup_mbic\n",
    "    best_scr_mBIC2, best_scr_sup2 = best_mbic2, best_sup_mbic2\n",
    "\n",
    "    for K in Ks_scr:\n",
    "        S = ranked[:K]\n",
    "        v1, v2 = _score_S(S)\n",
    "        if v1 < best_scr_mBIC - TOL:\n",
    "            best_scr_mBIC, best_scr_sup = v1, S.copy()\n",
    "        if v2 < best_scr_mBIC2 - TOL:\n",
    "            best_scr_mBIC2, best_scr_sup2 = v2, S.copy()\n",
    "\n",
    "    if best_scr_mBIC < best_mbic - TOL:\n",
    "        best_mbic, best_sup_mbic = best_scr_mBIC, best_scr_sup\n",
    "    if best_scr_mBIC2 < best_mbic2 - TOL:\n",
    "        best_mbic2, best_sup_mbic2 = best_scr_mBIC2, best_scr_sup2\n",
    "\n",
    "    # ------------------------------\n",
    "    # Residual-Refinement (linear & logistic) – getrennt für mBIC und mBIC2\n",
    "    # ------------------------------\n",
    "    def _fit_and_residuals(S: np.ndarray):\n",
    "        if S.size == 0:\n",
    "            if is_logistic:\n",
    "                p0 = float(np.clip(y.mean(), 1e-12, 1-1e-12))\n",
    "                r  = y - p0\n",
    "                return None, r, None\n",
    "            else:\n",
    "                return None, y, None\n",
    "        Xs = X[:, S]\n",
    "        if is_logistic:\n",
    "            mdl = LogisticRegression(penalty=None, solver=\"lbfgs\", max_iter=2000).fit(Xs, y.astype(int))\n",
    "            proba = np.clip(mdl.predict_proba(Xs)[:, 1], 1e-12, 1 - 1e-12)\n",
    "            r = y - proba\n",
    "            coef = mdl.coef_.ravel()\n",
    "            return mdl, r, coef\n",
    "        else:\n",
    "            mdl = LinearRegression().fit(Xs, y)\n",
    "            r = y - mdl.predict(Xs)\n",
    "            coef = mdl.coef_\n",
    "            return mdl, r, coef\n",
    "\n",
    "    if (best_sup_mbic.size < K_MAX) or (best_sup_mbic2.size < K_MAX):\n",
    "        max_local_steps = 6\n",
    "        cand_cap_add    = min(3 * K_MAX, 240)\n",
    "        cand_cap_swap   = min(2 * K_MAX, 180)\n",
    "\n",
    "        # ===========================\n",
    "        # PASS 1: mBIC verfeinern\n",
    "        # ===========================\n",
    "        if best_sup_mbic.size < K_MAX:\n",
    "            # (0) Backward-Prune (einmalig)\n",
    "            improved = True\n",
    "            while improved and best_sup_mbic.size > 0:\n",
    "                improved = False\n",
    "                S = best_sup_mbic\n",
    "                for i in range(S.size):\n",
    "                    S_try = np.delete(S, i)\n",
    "                    v_try, _ = _score_S(S_try)  # (mBIC, mBIC2)\n",
    "                    if v_try < best_mbic - TOL:\n",
    "                        best_mbic, best_sup_mbic = v_try, S_try\n",
    "                        improved = True\n",
    "                        break  # Neustart der Schleife mit neuem Support\n",
    "\n",
    "            # (A) Greedy Add (bis keine Verbesserung)\n",
    "            for _ in range(max_local_steps):\n",
    "                _, r, _ = _fit_and_residuals(best_sup_mbic)\n",
    "                scores_r = np.abs(X.T @ r)\n",
    "                S_set = set(best_sup_mbic.tolist())\n",
    "                cand = [j for j in np.argsort(scores_r)[::-1] if j not in S_set][:cand_cap_add]\n",
    "                improved = False\n",
    "                for j in cand:\n",
    "                    S_try = np.array(sorted(S_set | {j}), dtype=int)\n",
    "                    if S_try.size > K_MAX:\n",
    "                        continue\n",
    "                    v_try, _ = _score_S(S_try)\n",
    "                    if v_try < best_mbic - TOL:\n",
    "                        best_mbic, best_sup_mbic = v_try, S_try\n",
    "                        improved = True\n",
    "                        break\n",
    "                if not improved:\n",
    "                    break\n",
    "\n",
    "            # (B) Swap (klein gehalten)\n",
    "            mdl, r, beta = _fit_and_residuals(best_sup_mbic)\n",
    "            if beta is not None and beta.size > 0:\n",
    "                order_in = np.argsort(np.abs(beta))  # schwächste zuerst\n",
    "                scores_r = np.abs(X.T @ r)\n",
    "                S_set    = set(best_sup_mbic.tolist())\n",
    "                cand_out = order_in[: min(10, beta.size)]\n",
    "                cand_in  = [j for j in np.argsort(scores_r)[::-1] if j not in S_set][:cand_cap_swap]\n",
    "\n",
    "                improved = True\n",
    "                iter_swap = 0\n",
    "                while improved and iter_swap < 4:\n",
    "                    improved = False\n",
    "                    for idx_worst in cand_out:\n",
    "                        for j in cand_in:\n",
    "                            if j in S_set:\n",
    "                                continue\n",
    "                            S_try = best_sup_mbic.copy()\n",
    "                            S_try[idx_worst] = j\n",
    "                            S_try = np.array(sorted(set(S_try.tolist())), dtype=int)\n",
    "                            if S_try.size > K_MAX:\n",
    "                                continue\n",
    "                            v_try, _ = _score_S(S_try)\n",
    "                            if v_try < best_mbic - TOL:\n",
    "                                best_mbic, best_sup_mbic = v_try, S_try\n",
    "                                S_set = set(S_try.tolist())\n",
    "                                improved = True\n",
    "                                break\n",
    "                        if improved:\n",
    "                            break\n",
    "                    iter_swap += 1\n",
    "\n",
    "        # ===========================\n",
    "        # PASS 2: mBIC2 verfeinern\n",
    "        # ===========================\n",
    "        if best_sup_mbic2.size < K_MAX:\n",
    "            # (0) Backward-Prune (einmalig)\n",
    "            improved = True\n",
    "            while improved and best_sup_mbic2.size > 0:\n",
    "                improved = False\n",
    "                S2 = best_sup_mbic2\n",
    "                for i in range(S2.size):\n",
    "                    S2_try = np.delete(S2, i)\n",
    "                    _, v2_try = _score_S(S2_try)  # (mBIC, mBIC2)\n",
    "                    if v2_try < best_mbic2 - TOL:\n",
    "                        best_mbic2, best_sup_mbic2 = v2_try, S2_try\n",
    "                        improved = True\n",
    "                        break\n",
    "\n",
    "            # (A) Greedy Add (bis keine Verbesserung)\n",
    "            for _ in range(max_local_steps):\n",
    "                _, r2, _ = _fit_and_residuals(best_sup_mbic2)\n",
    "                scores_r2 = np.abs(X.T @ r2)\n",
    "                S2_set = set(best_sup_mbic2.tolist())\n",
    "                cand2 = [j for j in np.argsort(scores_r2)[::-1] if j not in S2_set][:cand_cap_add]\n",
    "                improved = False\n",
    "                for j in cand2:\n",
    "                    S2_try = np.array(sorted(S2_set | {j}), dtype=int)\n",
    "                    if S2_try.size > K_MAX:\n",
    "                        continue\n",
    "                    _, v2_try = _score_S(S2_try)\n",
    "                    if v2_try < best_mbic2 - TOL:\n",
    "                        best_mbic2, best_sup_mbic2 = v2_try, S2_try\n",
    "                        improved = True\n",
    "                        break\n",
    "                if not improved:\n",
    "                    break\n",
    "\n",
    "            # (B) Swap (klein gehalten)\n",
    "            mdl2, r2, beta2 = _fit_and_residuals(best_sup_mbic2)\n",
    "            if beta2 is not None and beta2.size > 0:\n",
    "                order_in2 = np.argsort(np.abs(beta2))  # schwächste zuerst\n",
    "                scores_r2 = np.abs(X.T @ r2)\n",
    "                S2_set    = set(best_sup_mbic2.tolist())\n",
    "                cand_out2 = order_in2[: min(10, beta2.size)]\n",
    "                cand_in2  = [j for j in np.argsort(scores_r2)[::-1] if j not in S2_set][:cand_cap_swap]\n",
    "\n",
    "                improved = True\n",
    "                iter_swap2 = 0\n",
    "                while improved and iter_swap2 < 4:\n",
    "                    improved = False\n",
    "                    for idx_worst2 in cand_out2:\n",
    "                        for j in cand_in2:\n",
    "                            if j in S2_set:\n",
    "                                continue\n",
    "                            S2_try = best_sup_mbic2.copy()\n",
    "                            S2_try[idx_worst2] = j\n",
    "                            S2_try = np.array(sorted(set(S2_try.tolist())), dtype=int)\n",
    "                            if S2_try.size > K_MAX:\n",
    "                                continue\n",
    "                            _, v2_try = _score_S(S2_try)\n",
    "                            if v2_try < best_mbic2 - TOL:\n",
    "                                best_mbic2, best_sup_mbic2 = v2_try, S2_try\n",
    "                                S2_set = set(S2_try.tolist())\n",
    "                                improved = True\n",
    "                                break\n",
    "                        if improved:\n",
    "                            break\n",
    "                    iter_swap2 += 1\n",
    "\n",
    "\n",
    "    # Finale Werte\n",
    "    mBIC_fin  = float(best_mbic)  if np.isfinite(best_mbic)  else np.nan\n",
    "    mBIC2_fin = float(best_mbic2) if np.isfinite(best_mbic2) else np.nan\n",
    "\n",
    "    return ModelSelResult(\n",
    "        mBIC=mBIC_fin,\n",
    "        mBIC2=mBIC2_fin,\n",
    "        model1=best_sup_mbic + 1,   # 1-basiert\n",
    "        model2=best_sup_mbic2 + 1,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab23960a-6c14-450a-9be4-8af9eb67d76e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ---------- Hilfsfunktionen --------------------------------\n",
    "def _l21_norm(w: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Zeilen‑weise L2‑Norm, dann Summe (||·||₂,₁).\"\"\"\n",
    "    return torch.norm(w, dim=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49547120",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def deep2stage(\n",
    "    y: np.ndarray,\n",
    "    X: np.ndarray,\n",
    "    model: str = \"linear\",          # \"linear\" | \"logistic\"\n",
    "    h: int = 128,                    # Hidden-Dim der Autoencoder-Bottleneck-Schicht\n",
    "    stage1_epochs: int = 250,\n",
    "    stage2_epochs: int = 250,\n",
    "    batch_size: int = 512,\n",
    "    λ: float = 0.1,                 # Rekonstruktionsgewicht (Stage 1)\n",
    "    α: float | None = None,         # L2,1-Penalty (Stage 2)\n",
    "    β: float = 0,                   # Frobenius-Decay (Stage 2)\n",
    "    lr: float = 1e-3,\n",
    "    K_max: int | None = None,       # Obergrenze Support-Größe für mBIC-Suche\n",
    "    patience: int = 12,\n",
    "    device: str | torch.device = \"cpu\",\n",
    ") -> ModelSelResult:\n",
    "    \"\"\"\n",
    "    Zweiseitiges DNN-Feature-Screening ohne Validierungsset.\n",
    "    Early-Stopping basiert auf dem (gemittelten) Train-Loss je Epoche.\n",
    "    \"\"\"\n",
    "    model = model.lower().strip()\n",
    "    is_logistic = (model == \"logistic\")\n",
    "    if α is None:\n",
    "        α = 1e-7 if is_logistic else 1e-4\n",
    "    if model not in (\"linear\", \"logistic\"):\n",
    "        raise ValueError(\"deep2stage: model muss 'linear' oder 'logistic' sein.\")\n",
    "\n",
    "    n, p = X.shape\n",
    "    if K_max is None:\n",
    "        K_MAX = min(int(0.5*n), int(0.5*p), 100)\n",
    "    else:\n",
    "        K_MAX = int(K_max)\n",
    "\n",
    "    # ----- Tensors (gesamter Datensatz als 'Train') -----\n",
    "    X_t = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "    if is_logistic:\n",
    "        y_int = y.astype(int)\n",
    "        y_t = torch.tensor(y_int, dtype=torch.long, device=device)\n",
    "    else:\n",
    "        y_t = torch.tensor(y, dtype=torch.float32, device=device)\n",
    "\n",
    "    # -------------- Stage 1 – (Supervised) Autoencoder -----\n",
    "    class SAE(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.enc = nn.Sequential(\n",
    "                nn.Linear(p, 256), nn.ReLU(),\n",
    "                nn.Linear(256, h)\n",
    "            )\n",
    "            self.dec = nn.Sequential(\n",
    "                nn.Linear(h, 256), nn.ReLU(),\n",
    "                nn.Linear(256, p)\n",
    "            )\n",
    "            self.head = nn.Linear(h, 2 if is_logistic else 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            z = self.enc(x)\n",
    "            x_hat = self.dec(z)\n",
    "            y_hat = self.head(z)\n",
    "            return z, x_hat, y_hat\n",
    "\n",
    "    sae = SAE().to(device)\n",
    "    opt1 = optim.RMSprop(sae.parameters(), lr=lr)\n",
    "\n",
    "    ds = DataLoader(TensorDataset(X_t, y_t),\n",
    "                    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    best_state1, best_loss1, no_imp1 = None, float(\"inf\"), 0\n",
    "    for _ in range(stage1_epochs):\n",
    "        sae.train()\n",
    "        running, nb = 0.0, 0\n",
    "        for xb, yb in ds:\n",
    "            opt1.zero_grad()\n",
    "            z, x_hat, y_hat = sae(xb)\n",
    "            recon = nn.functional.mse_loss(x_hat, xb)\n",
    "            if is_logistic:\n",
    "                sup = nn.functional.cross_entropy(y_hat, yb)\n",
    "            else:\n",
    "                sup = nn.functional.mse_loss(y_hat.squeeze(), yb)\n",
    "            loss = sup + λ * recon\n",
    "            loss.backward()\n",
    "            opt1.step()\n",
    "            running += float(loss.item())\n",
    "            nb += 1\n",
    "        epoch_loss = running / max(nb, 1)\n",
    "        if epoch_loss < best_loss1 - 1e-6:\n",
    "            best_loss1, no_imp1 = epoch_loss, 0\n",
    "            best_state1 = {k: v.detach().clone() for k, v in sae.state_dict().items()}\n",
    "        else:\n",
    "            no_imp1 += 1\n",
    "            if no_imp1 >= patience:\n",
    "                break  # Early-Stopping Stage 1 (Train-Loss)\n",
    "\n",
    "    if best_state1 is not None:\n",
    "        sae.load_state_dict(best_state1)\n",
    "\n",
    "    # Bottleneck-Features für alle Beobachtungen + Min-Max-Norm\n",
    "    with torch.no_grad():\n",
    "        x_encode = sae.enc(X_t)\n",
    "        x_encode = (x_encode - x_encode.min(0).values) / \\\n",
    "                   (x_encode.max(0).values - x_encode.min(0).values + 1e-8)\n",
    "\n",
    "    # -------------- Stage 2 – Regularisiertes 1-Hidden-Net --\n",
    "    class Student(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.fc1 = nn.Linear(p, h)\n",
    "            self.out = nn.Linear(h, h, bias=True)\n",
    "        def forward(self, x):\n",
    "            return self.out(torch.relu(self.fc1(x)))\n",
    "\n",
    "    stu = Student().to(device)\n",
    "    opt2 = optim.RMSprop(stu.parameters(), lr=lr)\n",
    "\n",
    "    best_state2, best_loss2, no_imp2 = None, float(\"inf\"), 0\n",
    "    for _ in range(stage2_epochs):\n",
    "        stu.train()\n",
    "        opt2.zero_grad()\n",
    "        x_hat_all = stu(X_t)\n",
    "        mse = nn.functional.mse_loss(x_hat_all, x_encode)\n",
    "        l21 = _l21_norm(stu.fc1.weight)\n",
    "        frob = stu.fc1.weight.norm()**2 + stu.out.weight.norm()**2\n",
    "        loss2 = mse + α * l21 + (β / 2) * frob\n",
    "        loss2.backward()\n",
    "        opt2.step()\n",
    "\n",
    "        train_loss = float(loss2.item())\n",
    "        if train_loss < best_loss2 - 1e-6:\n",
    "            best_loss2, no_imp2 = train_loss, 0\n",
    "            best_state2 = {k: v.detach().clone() for k, v in stu.state_dict().items()}\n",
    "        else:\n",
    "            no_imp2 += 1\n",
    "            if no_imp2 >= patience:\n",
    "                break  # Early-Stopping Stage 2 (Train-Loss)\n",
    "\n",
    "    if best_state2 is not None:\n",
    "        stu.load_state_dict(best_state2)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Feature-Scores → Ranking\n",
    "    # --------------------------------------------------------\n",
    "    with torch.no_grad():\n",
    "        W1 = stu.fc1.weight                                  # h × p\n",
    "        scores = torch.sum(W1 * W1, dim=0).cpu().numpy()     # diag(W₁ᵀW₁)\n",
    "    ranked = scores.argsort()[::-1]\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # mBIC / mBIC2 entlang der Top-k-Supports\n",
    "    # --------------------------------------------------------\n",
    "    best_BIC  = best_BIC2 = float(\"inf\")\n",
    "    best_sup1 = best_sup2 = np.array([], dtype=int)\n",
    "\n",
    "    n_float = float(n)\n",
    "    for k in range(0, min(K_MAX, p) + 1):\n",
    "        supp = ranked[:k]\n",
    "\n",
    "        if k == 0:\n",
    "            # ----- Log-Likelihood ohne Prädiktoren ---------------\n",
    "            if is_logistic:\n",
    "                eps = 1e-12\n",
    "                p_hat = np.clip(y_int.mean(), eps, 1 - eps)\n",
    "                ll = float((y_int * np.log(p_hat) + (1 - y_int) * np.log(1 - p_hat)).sum())\n",
    "            else:\n",
    "                sigma2_0 = np.var(y, ddof=0)\n",
    "                ll = float(-0.5 * n_float * np.log(sigma2_0 + 1e-12))\n",
    "            df = 0\n",
    "        else:\n",
    "            Xk = X[:, supp]\n",
    "            if is_logistic:\n",
    "                try:\n",
    "                    mdl = LogisticRegression(penalty=None, solver=\"lbfgs\", max_iter=2000).fit(Xk, y_int)\n",
    "                except Exception:\n",
    "                    # Fallback für ältere sklearn-Versionen\n",
    "                    mdl = LogisticRegression(penalty=\"l2\", C=1e12, solver=\"lbfgs\", max_iter=2000).fit(Xk, y_int)\n",
    "                # LL via proba + Clipping (deine Variante)\n",
    "                class1_col = int(np.where(mdl.classes_ == 1)[0][0])\n",
    "                proba = mdl.predict_proba(Xk)[:, class1_col]\n",
    "                proba = np.clip(proba, 1e-12, 1 - 1e-12)\n",
    "                ll = float((y_int * np.log(proba) + (1 - y_int) * np.log(1 - proba)).sum())\n",
    "            else:\n",
    "                mdl = LinearRegression().fit(Xk, y)\n",
    "                rss = float(((mdl.predict(Xk) - y) ** 2).sum())\n",
    "                sigma2 = rss / n_float\n",
    "                ll = float(-0.5 * n_float * np.log(sigma2 + 1e-12))\n",
    "            df = k\n",
    "\n",
    "        mBIC  = _mbic_bigstep(ll, n, k, p)\n",
    "        mBIC2 = _mbic2_bigstep(ll, n, k, p)\n",
    "\n",
    "        if mBIC < best_BIC:\n",
    "            best_BIC, best_sup1 = mBIC, supp.copy()\n",
    "        if mBIC2 < best_BIC2:\n",
    "            best_BIC2, best_sup2 = mBIC2, supp.copy()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Rückgabe (1-basiert)\n",
    "    # --------------------------------------------------------\n",
    "    return ModelSelResult(\n",
    "        mBIC  = float(best_BIC),\n",
    "        mBIC2 = float(best_BIC2),\n",
    "        model1 = best_sup1 + 1,\n",
    "        model2 = best_sup2 + 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f314c55b-ca83-45b7-8205-fa3f657dd5b0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def deep2stage_plus(\n",
    "    y: np.ndarray,\n",
    "    X: np.ndarray,\n",
    "    model: str = \"linear\",          # \"linear\" | \"logistic\"\n",
    "    h: int = 128,                    # Hidden-Dim der Autoencoder-Bottleneck-Schicht\n",
    "    stage1_epochs: int = 250,\n",
    "    stage2_epochs: int = 250,\n",
    "    batch_size: int = 512,\n",
    "    λ: float = 0.1,                 # Rekonstruktionsgewicht (Stage 1)\n",
    "    α: float | None = None,         # L2,1-Penalty (Stage 2)\n",
    "    β: float = 0,                   # Frobenius-Decay (Stage 2)\n",
    "    lr: float = 1e-3,\n",
    "    K_max: int | None = None,       # Obergrenze Support-Größe für mBIC-Suche\n",
    "    patience: int = 12,\n",
    "    device: str | torch.device = \"cpu\",\n",
    ") -> ModelSelResult:\n",
    "    \"\"\"\n",
    "    deep2stage + (Top-K Feinsuche, Korrelations-Screen, Residual-Refinement).\n",
    "    Early-Stopping bleibt auf Train-Loss je Epoche.\n",
    "    \"\"\"\n",
    "    model = model.lower().strip()\n",
    "    is_logistic = (model == \"logistic\")\n",
    "    if α is None:\n",
    "        α = 1e-7 if is_logistic else 1e-4\n",
    "    if model not in (\"linear\", \"logistic\"):\n",
    "        raise ValueError(\"deep2stage_plus: model muss 'linear' oder 'logistic' sein.\")\n",
    "\n",
    "    n, p = X.shape\n",
    "    if K_max is None:\n",
    "        K_MAX = min(int(0.5*n), int(0.5*p), 100)\n",
    "    else:\n",
    "        K_MAX = int(K_max)\n",
    "    K_hi = min(K_MAX, p)\n",
    "    TOL = 1e-9\n",
    "\n",
    "    # ----- Tensors -----\n",
    "    X_t = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "    if is_logistic:\n",
    "        y_int = y.astype(int)\n",
    "        y_t = torch.tensor(y_int, dtype=torch.long, device=device)\n",
    "    else:\n",
    "        y_t = torch.tensor(y, dtype=torch.float32, device=device)\n",
    "\n",
    "    # -------------- Stage 1 – (Supervised) Autoencoder -----\n",
    "    class SAE(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.enc = nn.Sequential(\n",
    "                nn.Linear(p, 256), nn.ReLU(),\n",
    "                nn.Linear(256, h)\n",
    "            )\n",
    "            self.dec = nn.Sequential(\n",
    "                nn.Linear(h, 256), nn.ReLU(),\n",
    "                nn.Linear(256, p)\n",
    "            )\n",
    "            self.head = nn.Linear(h, 2 if is_logistic else 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            z = self.enc(x)\n",
    "            x_hat = self.dec(z)\n",
    "            y_hat = self.head(z)\n",
    "            return z, x_hat, y_hat\n",
    "\n",
    "    sae = SAE().to(device)\n",
    "    opt1 = optim.RMSprop(sae.parameters(), lr=lr)\n",
    "    ds = DataLoader(TensorDataset(X_t, y_t),\n",
    "                    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    best_state1, best_loss1, no_imp1 = None, float(\"inf\"), 0\n",
    "    for _ in range(stage1_epochs):\n",
    "        sae.train()\n",
    "        running, nb = 0.0, 0\n",
    "        for xb, yb in ds:\n",
    "            opt1.zero_grad()\n",
    "            z, x_hat, y_hat = sae(xb)\n",
    "            recon = nn.functional.mse_loss(x_hat, xb)\n",
    "            if is_logistic:\n",
    "                sup = nn.functional.cross_entropy(y_hat, yb)\n",
    "            else:\n",
    "                sup = nn.functional.mse_loss(y_hat.squeeze(), yb)\n",
    "            loss = sup + λ * recon\n",
    "            loss.backward()\n",
    "            opt1.step()\n",
    "            running += float(loss.item()); nb += 1\n",
    "        epoch_loss = running / max(nb, 1)\n",
    "        if epoch_loss < best_loss1 - 1e-6:\n",
    "            best_loss1, no_imp1 = epoch_loss, 0\n",
    "            best_state1 = {k: v.detach().clone() for k, v in sae.state_dict().items()}\n",
    "        else:\n",
    "            no_imp1 += 1\n",
    "            if no_imp1 >= patience:\n",
    "                break\n",
    "\n",
    "    if best_state1 is not None:\n",
    "        sae.load_state_dict(best_state1)\n",
    "\n",
    "    # Bottleneck-Features (Min-Max-Norm)\n",
    "    with torch.no_grad():\n",
    "        x_encode = sae.enc(X_t)\n",
    "        x_encode = (x_encode - x_encode.min(0).values) / \\\n",
    "                   (x_encode.max(0).values - x_encode.min(0).values + 1e-8)\n",
    "\n",
    "    # -------------- Stage 2 – Regularisiertes 1-Hidden-Net --\n",
    "    class Student(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.fc1 = nn.Linear(p, h)\n",
    "            self.out = nn.Linear(h, h, bias=True)\n",
    "        def forward(self, x):\n",
    "            return self.out(torch.relu(self.fc1(x)))\n",
    "\n",
    "    stu = Student().to(device)\n",
    "    opt2 = optim.RMSprop(stu.parameters(), lr=lr)\n",
    "\n",
    "    best_state2, best_loss2, no_imp2 = None, float(\"inf\"), 0\n",
    "    for _ in range(stage2_epochs):\n",
    "        stu.train()\n",
    "        opt2.zero_grad()\n",
    "        x_hat_all = stu(X_t)\n",
    "        mse = nn.functional.mse_loss(x_hat_all, x_encode)\n",
    "        l21 = _l21_norm(stu.fc1.weight)\n",
    "        frob = stu.fc1.weight.norm()**2 + stu.out.weight.norm()**2\n",
    "        loss2 = mse + α * l21 + (β / 2) * frob\n",
    "        loss2.backward()\n",
    "        opt2.step()\n",
    "\n",
    "        train_loss = float(loss2.item())\n",
    "        if train_loss < best_loss2 - 1e-6:\n",
    "            best_loss2, no_imp2 = train_loss, 0\n",
    "            best_state2 = {k: v.detach().clone() for k, v in stu.state_dict().items()}\n",
    "        else:\n",
    "            no_imp2 += 1\n",
    "            if no_imp2 >= patience:\n",
    "                break\n",
    "\n",
    "    if best_state2 is not None:\n",
    "        stu.load_state_dict(best_state2)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Feature-Scores → Ranking (Importances)\n",
    "    # --------------------------------------------------------\n",
    "    with torch.no_grad():\n",
    "        W1 = stu.fc1.weight                                  # h × p\n",
    "        scores = torch.sum(W1 * W1, dim=0).cpu().numpy()     # diag(W₁ᵀW₁)\n",
    "    ranked = scores.argsort()[::-1]\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # (Baseline) mBIC / mBIC2 entlang der Top-k-Supports\n",
    "    # --------------------------------------------------------\n",
    "    def _ll_full(supp: np.ndarray) -> tuple[float, int]:\n",
    "        k = supp.size\n",
    "        if k == 0:\n",
    "            if is_logistic:\n",
    "                eps = 1e-12\n",
    "                p_hat = float(np.clip(y.mean(), eps, 1 - eps))\n",
    "                ll = float((y * np.log(p_hat) + (1 - y) * np.log(1 - p_hat)).sum())\n",
    "            else:\n",
    "                sigma2_0 = max(float(np.var(y, ddof=0)), 1e-12)\n",
    "                ll = -0.5 * n * np.log(sigma2_0)\n",
    "            return ll, 0\n",
    "        Xk = X[:, supp]\n",
    "        if is_logistic:\n",
    "            try:\n",
    "                mdl = LogisticRegression(penalty=None, solver=\"lbfgs\", max_iter=2000).fit(Xk, y.astype(int))\n",
    "            except Exception:\n",
    "                mdl = LogisticRegression(penalty=\"l2\", C=1e12, solver=\"lbfgs\", max_iter=2000).fit(Xk, y.astype(int))\n",
    "            proba = np.clip(mdl.predict_proba(Xk)[:, 1], 1e-12, 1 - 1e-12)\n",
    "            ll = float((y * np.log(proba) + (1 - y) * np.log(1 - proba)).sum())\n",
    "        else:\n",
    "            mdl = LinearRegression().fit(Xk, y)\n",
    "            rss = float(((mdl.predict(Xk) - y) ** 2).sum())\n",
    "            sigma2 = max(rss / n, 1e-12)\n",
    "            ll = -0.5 * n * np.log(sigma2)\n",
    "        return ll, k\n",
    "\n",
    "    best_mbic  = float(\"inf\"); best_sup_mbic  = np.array([], dtype=int)\n",
    "    best_mbic2 = float(\"inf\"); best_sup_mbic2 = np.array([], dtype=int)\n",
    "\n",
    "    for k in range(0, min(K_MAX, p) + 1):\n",
    "        supp = ranked[:k]\n",
    "        ll, df = _ll_full(supp)\n",
    "        mbic  = _mbic_bigstep(ll, n, df, p)\n",
    "        mbic2 = _mbic2_bigstep(ll, n, df, p)\n",
    "        if mbic < best_mbic:\n",
    "            best_mbic, best_sup_mbic = mbic, supp.copy()\n",
    "        if mbic2 < best_mbic2:\n",
    "            best_mbic2, best_sup_mbic2 = mbic2, supp.copy()\n",
    "\n",
    "    # ========================================================\n",
    "    # PLUS-TEIL 1: Top-K auf Basis Importances + Feinsuche\n",
    "    # (feature_importances_ → hier: scores aus W1; Fallback: |X^Ty|)\n",
    "    # ========================================================\n",
    "    try:\n",
    "        imp_final = np.asarray(scores)\n",
    "        rank_theta = np.argsort(np.abs(imp_final))[::-1]\n",
    "    except Exception:\n",
    "        rank_theta = np.argsort(np.abs(X.T @ (y if not is_logistic else y.astype(float))))[::-1]\n",
    "\n",
    "    def _score_S(S: np.ndarray):\n",
    "        ll, kk = _ll_full(S)\n",
    "        return _mbic_bigstep(ll, n, kk, p), _mbic2_bigstep(ll, n, kk, p)\n",
    "\n",
    "    def _score_K(K: int):\n",
    "        S = rank_theta[:K]\n",
    "        v1, v2 = _score_S(S)\n",
    "        return v1, v2, S\n",
    "\n",
    "    K_seed = int(best_sup_mbic.size) if best_sup_mbic.size > 0 else min(8, K_hi)\n",
    "    WINDOW, STEP = 10, 2\n",
    "    K_low  = max(1, K_seed - WINDOW)\n",
    "    K_up   = min(K_seed + WINDOW, K_hi)\n",
    "\n",
    "    best_theta_mBIC, best_theta_mBIC2 = best_mbic, best_mbic2\n",
    "    best_theta_sup = best_sup_mbic.copy()\n",
    "    K_best = K_seed\n",
    "\n",
    "    for K in range(K_low, K_up + 1, STEP):\n",
    "        v1, v2, S = _score_K(K)\n",
    "        if v1 < best_theta_mBIC - TOL:\n",
    "            best_theta_mBIC, best_theta_mBIC2, best_theta_sup, K_best = v1, v2, S.copy(), K\n",
    "\n",
    "    for K in range(max(1, K_best - 3), min(K_hi, K_best + 3) + 1):\n",
    "        v1, v2, S = _score_K(K)\n",
    "        if v1 < best_theta_mBIC - TOL:\n",
    "            best_theta_mBIC, best_theta_mBIC2, best_theta_sup, K_best = v1, v2, S.copy(), K\n",
    "\n",
    "    if best_theta_mBIC  < best_mbic  - TOL: best_mbic,  best_sup_mbic  = best_theta_mBIC,  best_theta_sup\n",
    "    if best_theta_mBIC2 < best_mbic2 - TOL: best_mbic2, best_sup_mbic2 = best_theta_mBIC2, best_theta_sup\n",
    "\n",
    "    # ========================================================\n",
    "    # PLUS-TEIL 2: Großer Korrelations-Screen\n",
    "    # ========================================================\n",
    "    K_screen = min(K_MAX, 70)\n",
    "    scores_glob = np.abs(X.T @ (y if not is_logistic else y.astype(float)))\n",
    "    ranked_scr = np.argsort(scores_glob)[::-1][:K_screen]\n",
    "\n",
    "    Ks_scr = [1]\n",
    "    while Ks_scr[-1] < K_screen:\n",
    "        Ks_scr.append(min(K_screen, Ks_scr[-1] * 2))\n",
    "    if best_sup_mbic.size > 0:\n",
    "        Ks_scr.append(min(K_screen, best_sup_mbic.size))\n",
    "    Ks_scr = sorted(set(Ks_scr))\n",
    "\n",
    "    best_scr_mBIC,  best_scr_sup  = best_mbic,  best_sup_mbic\n",
    "    best_scr_mBIC2, best_scr_sup2 = best_mbic2, best_sup_mbic2\n",
    "\n",
    "    for K in Ks_scr:\n",
    "        S = ranked_scr[:K]\n",
    "        v1, v2 = _score_S(S)\n",
    "        if v1 < best_scr_mBIC - TOL:\n",
    "            best_scr_mBIC, best_scr_sup = v1, S.copy()\n",
    "        if v2 < best_scr_mBIC2 - TOL:\n",
    "            best_scr_mBIC2, best_scr_sup2 = v2, S.copy()\n",
    "\n",
    "    if best_scr_mBIC < best_mbic - TOL:\n",
    "        best_mbic, best_sup_mbic = best_scr_mBIC, best_scr_sup\n",
    "    if best_scr_mBIC2 < best_mbic2 - TOL:\n",
    "        best_mbic2, best_sup_mbic2 = best_scr_mBIC2, best_scr_sup2\n",
    "\n",
    "    # ========================================================\n",
    "    # PLUS-TEIL 3: Residual-Refinement (linear & logistic)\n",
    "    # ========================================================\n",
    "    def _fit_and_residuals(S: np.ndarray):\n",
    "        if S.size == 0:\n",
    "            if is_logistic:\n",
    "                p0 = float(np.clip(y.mean(), 1e-12, 1-1e-12))\n",
    "                r  = y - p0\n",
    "                return None, r, None\n",
    "            else:\n",
    "                return None, y, None\n",
    "        Xs = X[:, S]\n",
    "        if is_logistic:\n",
    "            mdl = LogisticRegression(penalty=None, solver=\"lbfgs\", max_iter=2000).fit(Xs, y.astype(int))\n",
    "            proba = np.clip(mdl.predict_proba(Xs)[:, 1], 1e-12, 1 - 1e-12)\n",
    "            r = y - proba\n",
    "            coef = mdl.coef_.ravel()\n",
    "            return mdl, r, coef\n",
    "        else:\n",
    "            mdl = LinearRegression().fit(Xs, y)\n",
    "            r = y - mdl.predict(Xs)\n",
    "            coef = mdl.coef_\n",
    "            return mdl, r, coef\n",
    "\n",
    "    if best_sup_mbic.size < K_MAX:\n",
    "        max_local_steps = 6\n",
    "        cand_cap_add    = min(3 * K_MAX, 240)\n",
    "        cand_cap_swap   = min(2 * K_MAX, 180)\n",
    "\n",
    "        # (0) Backward-Prune\n",
    "        improved = True\n",
    "        while improved and best_sup_mbic.size > 0:\n",
    "            improved = False\n",
    "            S = best_sup_mbic\n",
    "            for i in range(S.size):\n",
    "                S_try = np.delete(S, i)\n",
    "                v_try, _ = _score_S(S_try)\n",
    "                if v_try < best_mbic - TOL:\n",
    "                    best_mbic, best_sup_mbic = v_try, S_try\n",
    "                    improved = True\n",
    "                    break\n",
    "\n",
    "        # (A) Greedy Add\n",
    "        for _ in range(max_local_steps):\n",
    "            _, r, _ = _fit_and_residuals(best_sup_mbic)\n",
    "            scores_r = np.abs(X.T @ r)\n",
    "            S_set = set(best_sup_mbic.tolist())\n",
    "            cand = [j for j in np.argsort(scores_r)[::-1] if j not in S_set][:cand_cap_add]\n",
    "            improved = False\n",
    "            for j in cand:\n",
    "                S_try = np.array(sorted(S_set | {j}), dtype=int)\n",
    "                if S_try.size > K_MAX:\n",
    "                    continue\n",
    "                v_try, _ = _score_S(S_try)\n",
    "                if v_try < best_mbic - TOL:\n",
    "                    best_mbic, best_sup_mbic = v_try, S_try\n",
    "                    improved = True\n",
    "                    break\n",
    "            if not improved:\n",
    "                break\n",
    "\n",
    "        # (B) Swap (klein gehalten)\n",
    "        mdl, r, beta = _fit_and_residuals(best_sup_mbic)\n",
    "        if beta is not None and beta.size > 0:\n",
    "            order_in = np.argsort(np.abs(beta))  # schwächste zuerst\n",
    "            scores_r = np.abs(X.T @ r)\n",
    "            S_set    = set(best_sup_mbic.tolist())\n",
    "            cand_out = order_in[: min(10, beta.size)]\n",
    "            cand_in  = [j for j in np.argsort(scores_r)[::-1] if j not in S_set][:cand_cap_swap]\n",
    "\n",
    "            improved = True\n",
    "            iter_swap = 0\n",
    "            while improved and iter_swap < 4:\n",
    "                improved = False\n",
    "                for idx_worst in cand_out:\n",
    "                    rem = best_sup_mbic[idx_worst]\n",
    "                    for j in cand_in:\n",
    "                        if j in S_set:\n",
    "                            continue\n",
    "                        S_try = best_sup_mbic.copy()\n",
    "                        S_try[idx_worst] = j\n",
    "                        S_try = np.array(sorted(set(S_try.tolist())), dtype=int)\n",
    "                        if S_try.size > K_MAX:\n",
    "                            continue\n",
    "                        v_try, _ = _score_S(S_try)\n",
    "                        if v_try < best_mbic - TOL:\n",
    "                            best_mbic, best_sup_mbic = v_try, S_try\n",
    "                            S_set = set(S_try.tolist())\n",
    "                            improved = True\n",
    "                            break\n",
    "                    if improved:\n",
    "                        break\n",
    "                iter_swap += 1\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Rückgabe (1-basiert)\n",
    "    # --------------------------------------------------------\n",
    "    mBIC_fin  = float(best_mbic)  if np.isfinite(best_mbic)  else np.nan\n",
    "    mBIC2_fin = float(best_mbic2) if np.isfinite(best_mbic2) else np.nan\n",
    "\n",
    "    return ModelSelResult(\n",
    "        mBIC  = mBIC_fin,\n",
    "        mBIC2 = mBIC2_fin,\n",
    "        model1 = best_sup_mbic + 1,\n",
    "        model2 = best_sup_mbic2 + 1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ffbdf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Öffentliche API\n",
    "__all__ = [\n",
    "    \"ModelSelResult\",\n",
    "    \"stepwise_plain\",\n",
    "    \"stepwise_reduced\",\n",
    "    \"stepwise_ff\",\n",
    "    \"L0opt_CD\",\n",
    "    \"L0opt_CDPSI\",\n",
    "    \"Select_GSDAR\",\n",
    "    \"lassonet\",\n",
    "    \"lassonet_plus\",\n",
    "    \"deep2stage\", \n",
    "    \"deep2stage_plus\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc35024-ed66-4d88-b672-5c1a85d3aeb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53f3993-c402-4fca-a7ce-3fc224418d55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
